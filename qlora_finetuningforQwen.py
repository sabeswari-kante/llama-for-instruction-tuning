# -*- coding: utf-8 -*-
"""QLora_llm_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11uZcEcnyokRG2fWRkdK-LAyxozfmeHi1
"""

!pip install -q transformers datasets accelerate peft bitsandbytes trl

"""**Define Model**"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# Specify the pre-trained model name
model_name = "Qwen/Qwen3-0.6B" # Example: Llama-2 7B Chat model

# Configure quantization with BitsAndBytesConfig
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,                # Enable 4-bit quantization
    bnb_4bit_quant_type="nf4",        # Use NF4 (Normal Float 4) quantization type
    bnb_4bit_compute_dtype=torch.bfloat16, # Set compute dtype to bfloat16 for speed
    bnb_4bit_use_double_quant=True,  # Enable nested quantization for more memory saving
)

"""Load Model with QLora"""

# Load the model with quantization configurat ion
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quantization_config,
    device_map="auto", # Automatically distribute model layers across available GPUs/CPU
    trust_remote_code=True, # Trust code execution from the model hub (use with caution)
    low_cpu_mem_usage=True
    )

"""**Load the tokenizer associated with the model**"""

#
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
# Set padding token if not already set (common requirement for training)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right" # Typically right padding for causal LMs

print(f"Model loaded: {model_name}")
print(f"Model memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB")

"""**Load Dataset**"""

from datasets import load_dataset

# Load a subset of the dataset
dataset_name = "databricks/databricks-dolly-15k"
dataset = load_dataset(dataset_name, split="train[:500]") # Using a small slice for demonstration

dataset.data

"""**Define a function to format the prompts**"""

def format_prompt(example):
    # Simple instruction-following format
    instruction = example.get("instruction", "")
    context = example.get("context", "")
    response = example.get("response", "")

    if context:
        prompt = f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Input:
{context}

### Response:
{response}"""
    else:
        prompt = f"""Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Response:
{response}"""
    # We need to return this in a column named 'text' for SFTTrainer
    return {"text": prompt}

# Apply the formatting function
formatted_dataset = dataset.map(format_prompt)

print("Sample formatted prompt:")
print(formatted_dataset[0]['text'])

# Prepare the model for k-bit training (gradient checkpointing, layer norm scaling)
model = prepare_model_for_kbit_training(model)

# Configure LoRA
lora_config = LoraConfig(
    r=16,                         # Rank of the update matrices (higher value = more parameters)
    lora_alpha=32,                # LoRA scaling factor (alpha/r controls the magnitude)
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], # Modules to apply LoRA to (specific to Llama-2 architecture)
    lora_dropout=0.05,            # Dropout probability for LoRA layers
    bias="none",                  # Do not train bias parameters
    task_type="CAUSAL_LM"         # Specify the task type
)

# Apply LoRA configuration to the quantized model
model = get_peft_model(model, lora_config)

# Print the percentage of trainable parameters
model.print_trainable_parameters()

import transformers
from trl import SFTTrainer

# Configure Training Arguments
training_args = transformers.TrainingArguments(
    output_dir="./qlora_finetuned_model",      # Directory to save checkpoints and logs
    per_device_train_batch_size=4,          # Batch size per GPU
    gradient_accumulation_steps=4,          # Accumulate gradients over 4 steps (effective batch size = 4 * 4 = 16)
    learning_rate=2e-4,                     # Learning rate
    num_train_epochs=2,                     # Number of training epochs (adjust based on dataset size)
    logging_steps=20,                       # Log training metrics every 20 steps
    save_steps=50,                          # Save checkpoints every 50 steps
    fp16=True,                              # Enable mixed-precision training (or bf16=True if supported)
    optim="paged_adamw_8bit",               # Use paged AdamW optimizer for memory efficiency
    lr_scheduler_type="cosine",             # Learning rate scheduler type
    warmup_ratio=0.03,                      # Warmup ratio for learning rate scheduler
    report_to="none"                        # Disable reporting to services like Weights & Biases for this example
                                        # SFTTrainer specific arguments
                                       # Maximum sequence length for packing
                                     # The column name containing the formatted text in the dataset

    )

# Initialize the SFTTrainer
trainer = SFTTrainer(
    model=model,                         # The PEFT-wrapped, quantized model
    train_dataset=formatted_dataset,     # The formatted training dataset
    args=training_args,                  # Training arguments
    peft_config=lora_config           # The LoRA configuration
    # Optional: You can add packing=True for increased efficiency, but requires careful sequence length handling
)

print("Starting QLoRA fine-tuning...")
trainer.train()
print("Training finished.")

# Define the path to save the adapter weights
adapter_output_dir = "./qlora_adapter_weights"

# Save the LoRA adapter weights
trainer.save_model(adapter_output_dir)
# Alternatively: model.save_pretrained(adapter_output_dir)

print(f"QLoRA adapter weights saved to: {adapter_output_dir}")

"""Prediction

"""

from peft import PeftModel
import time

# Reload the base quantized model (if not already in memory)
# Ensure you use the same quantization_config as during training
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quantization_config,
    device_map="auto",
    trust_remote_code=True,
    low_cpu_mem_usage=True

)

# Load the PEFT model by merging the adapter weights into the base model
model_tuned = PeftModel.from_pretrained(base_model, adapter_output_dir)
model_tuned = model_tuned.eval() # Set the model to evaluation mode

# --- Inference Example ---

# Prepare a sample prompt (using the same format as training, but without the response)
instruction = "What is the difference between LoRA and QLoRA?"
prompt_template = f"""Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Response:
"""

# Tokenize the input prompt
inputs = tokenizer(prompt_template, return_tensors="pt").to(model_tuned.device)

print("\n--- Generating Response ---")
start_time = time.time()

# Generate text using the fine-tuned model
with torch.no_grad(): # Disable gradient calculations for inference
    outputs = model_tuned.generate(
        **inputs,
        max_new_tokens=500,       # Maximum number of new tokens to generate
        do_sample=True,           # Enable sampling
        temperature=0.7,          # Control randomness (lower = more deterministic)
        top_k=50,                 # Consider top k tokens for sampling
        top_p=0.95,               # Use nucleus sampling (cumulative probability cutoff)
        eos_token_id=tokenizer.eos_token_id # Stop generation upon encountering the EOS token
    )

end_time = time.time()

# Decode the generated tokens
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(response)
print(f"\nGeneration time: {end_time - start_time:.2f} seconds")

print('Done')

